---
title: "fNIRS Parkinson DT study - notebook"
bibliography: C:/pandoc/zotero_lib.bib
format: 
  html:
    code-fold: true
    code-summary: "Show the code"
    embed-resources: true
    toc: true
    toc-depth: 2
    toc-expand: 2
    theme: flatly
    output-file: index.html
  docx:
    toc: false
  
editor: source
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=10, fig.height=5) 

# load library
library(lme4)
library(jtools)
library(dplyr)
library(plyr, include.only = c("revalue"))
library(tidyr)
library(purrr)
library(stringr)
library(forcats)
library(arsenal)

library(ggpubr)
library(ggplot2)
library(patchwork)
library(cowplot)
library(ggiraph)
library(ggpmisc)

library(gt)
library(gtExtras)
library(ggstatsplot)

```

# Introduction

This document is intended to be used as an "open lab notebook" for an fNIRS study on dual-task walking, analyzing an existing dataset, the Park-MOVE dataset collected between 2021 and 2023 [@franzenParkMOVEFNIRSStudy2023]. Note that while the data is in a central repository, access to the data is restricted and regulated by data transfer agreements.

To provide transparent and systematic support for the findings of the study, this document is updated throughout the study and contains:

-   Background and methods

<!-- -->

-   Analysis plan

-   Sensitivity analysis

-   Performed analyses

-   Model validation and robustness analyses

This document is stored on GitHub pages: <https://alkvi.github.io/fnirs_stroop_study/>

Version 1 was stored at OSF: <https://osf.io/m7tx6/>

How the dataset used in this study was prepared can be found in: <https://github.com/alkvi/fnirs_dataset_preparation>

## Version history

| Revision | Date | Update |
|------------------------|------------------------|------------------------|
| 1 | 2024-12-09 | First version, with analysis plan |
| 2 | 2025-02-12 | Analyses performed, model diagnostics and outlier analysis done. One subject included in rev1 was excluded (after re-evaluating exclusion criteria). Moved format to Quarto. |
| 3 | 2025-02-14 | Update aim 3 to control for education, add details on missing data |
| 4 | 2025-02-19 | Changed correlation method in aim 3 behavioral part to Spearman (more robust), added exploratory analysis of PFC activation difference between groups, added signal quality metrics, used z-score normalization of covariates for more evenly scaled betas (does not affect statistical test values which are the same as before) |

: Version history

# Notebook rationale

Since we already have a collected dataset, on which we have already run analyses (albeit different analyses), we can take inspiration from Daniël Lakens, for example in [@lakens2024]:

> We propose that the ‘open lab notebook’ terminology is more appropriate for studies where data already exist. \[…\] Open lab notebooks focus on robustness analyses, sensitivity analyses, and triangulation to provide support for claims (Munafò and Smith 2018).

Or in the corresponding blog post (<http://daniellakens.blogspot.com/2024/07/new-paper-benefits-of-preregistration.html)>:

> That means we also do not expect people who have different epistemological philosophies to preregister – nor is it a logical solution for exploratory research, or certain types of secondary data analysis. We feel it is important to point this out, because there are alternative approaches to argue a test is severe that are better suited for those studies: open lab notebooks, sensitivity analyses, robustness checks, independent replication. It is always important to use the right tool for the job - we do not want preregistration to be mindlessly overused.

So we can be transparent in documenting how the data has already been used, publishing our final analysis plan on OSF, updating it along the way with steps performed as an open lab notebook (although not publishing any data), taking into consideration what we can do to support the severity of tests that will be performed.

# Background

Walking can be affected by concurrent cognitive tasks, leading to poorer performance such as slower gait speed [@smith2016], and increased gait variability [@smith2017]. While this is true in both aging [@beurskens2012] and various neurodegenerative diseases [@mcisaac2018], people with Parkinson's disease especially have been shown to have walking impairments elicited by dual-task gait [@nieuwhof2017], leading to fall risk and poorer quality of life [@kelly2012].

One contributor to dual-task motor impairments and fall risk could be a "posture second" strategy, based on an observation that people with Parkinson's disease have worse gait and balance while performing cognitive tasks while walking due to focusing on the cognitive task instead of maintaining balance [@bloem2006]. However, in a later review of dual-task walking in Parkinson's disease this observation was only demonstrated in one study out of several, and so remains unclear [@kelly2012].

Another contributor to dual-task impairments is thought to be excessive attentional demands of normal gait due to reduced locomotor automaticity [@clark2015]. This is supported by the observation that dual-task impairments increase with cognitive load [@goh2021; @maclean2017] and are exacerbated by reduced executive function [@yogev-seligmann2008].

The indirect [@hamacher2015; @herold2017] or executive locomotor [@clark2015; @lafougère2010] pathway involving the prefrontal cortex is thought to be involved in dual-task gait impairments. Specifically, this pathway may be relied upon more heavily in people with dual-task impairments due to both reduced locomotor automaticity and increased executive demands of the concurrent task.

One model among several [@festini2018] for explaining the elevated attentional demand during normal walking in aging is the CRUNCH model [@reuter-lorenz2008]. This model aims to explain functional brain imaging observations of overactivation in older adults compared to younger adults during cognitive and motoric tasks, hypothesizing that overactivation of a region or network in the brain might be due to age-related atrophy [@reuter-lorenz2008]. Such compensation has been further divided into upregulation, selection, and reorganization [@cabeza2018], where reorganization of activity from automatic circuits to volitional prefrontal circuits during normal walking might explain increased dual-task interference.

The CRUNCH model could also be applied to pathological aging [@bunzeck2024] including Parkinson’s disease [@kim2022], with an important additional observation of compensatory mechanisms breaking down at a certain tipping point of neural degeneration or task demand [@bunzeck2024].

However, there are not many studies looking at this mechanism in Parkinson’s disease. While some studies do look at brain activity during dual-tasking in Parkinson’s disease [@kahya2019], there are only a few that investigate neural correlates of measured brain activity [@kim2022]. Adding to the literature on neural correlates and putting results in the context of theoretical models would be a worthwhile effort.

To better understand prefrontal activity during walking and dual-task walking in older adults and people with PD, we set out to test models relating measures of gait automaticity to prefrontal activity, including testing whether cognitive capacity acts as a moderator.

# Methods

This study uses data from the Park-MOVE dataset, collected between 2021 and 2023 [@franzenParkMOVEFNIRSStudy2023]. For this dataset, experiments took place across two sessions at the uMOVE core facility, Karolinska University Hospital, Solna, Stockholm. During the experimental sessions, clinical tests of balance, disease severity and a neuropsychological test battery were performed, along with fNIRS measurement during a block-based complex walking protocol. Participants performed the clinical tests and neuropsychological tests during one session and the fNIRS measurement during another, with approximately one week between the sessions.

Clinical tests of balance were performed for the older adult and PD groups with the Mini-Balance Evaluation Systems Test (Mini-BESTest) [@dicarlo2016] and disease severity for the PD group with the Movement Disorder Society-sponsored revision of the Unified Parkinson’s Disease Rating Scale (MDS-UPDRS) [@goetz2008].

The neuropsychological test battery comprised the following tests: The Color-Word Interference Test (CWIT) part III from the Delis-Kaplan Executive Function System (D-KEFS) [@delis2001delis], Verbal Fluency part I-III (from D-KEFS), Trail Making Test (TMT) part II and IV (from D-KEFS) and Ray Auditory Verbal Learning Test (RAVLT) [@schmidt1996].

The fNIRS system used was a NIRSport2 (NIRx) with 8 sources and 8 detectors, with short-separation detection channels for each source to allow for removing superficial blood flow changes in the signal. The optodes transmitted light at 760 and 850 nm, and the sampling frequency was 10 Hz. Data was captured using Aurora (NIRx) (v.1.4). The optodes were fitted to a cap according to the international 10–20 system and placed over the prefrontal area. Caps were chosen in accordance with the head sizes of participants.

The walking protocol analyzed in this study contained tasks of straight walking (Walking ST), standing still while performing an auditory Stroop task (Standing ST) and straight walking while performing the auditory Stroop task (Walking DT). For the straight walking condition, participants were asked to walk straight at a self-selected speed to a cone 30 m from the starting cone and back. The auditory Stroop task consisted of the Swedish words for high and low in a congruent or incongruent high and low pitch. Words were presented to the participants through wireless headphones. Participants were instructed to respond verbally, as fast and correctly as possible, to the corresponding pitch irrespective of the words presented. The responses were recorded to analyze task accuracy and response time.

During each block with an auditory Stroop task, a total of 7-word prompts of high or low were presented in a predetermined randomized order. Participants were instructed to pay equal attention to both tasks when dual-tasking. Block length for stimuli was 20 s long, followed by 15 s of rest period to allow for a baseline measure. Each block condition was performed 6 times (e.g., 6 blocks of walking straight) with the time being approximately 12 minutes to complete the protocol.

Gait parameters (e.g., step time and walking speed) were collected using three wireless inertial sensors (Opal, APDM Inc.) positioned over the lumbar and on top of each foot near the ankle. Raw data in each block was analyzed in the python Gaitmap library [@10411039a]. Step time variability was calculated based on the standard deviation of left and right steps according to [@galna2013]. Outliers in gait variables due to unexpected behavior in the protocol (e. g., stopping during a walking block) were excluded from analysis. Two strides were excluded from the start and end of each block to get steady state gait [@miller1996].

Analysis of fNIRS data was performed in the MATLAB NIRS AnalyzIR toolbox [@santosa2018]. The raw optical density fNIRS data was converted into ΔHbO2 and ΔHHb using the modified Beer-Lambert law [@delpy1988] with the differential path-length factor (DPF) dependent on age [@scholkmann2013]. The first level (subject level) analysis employed a general linear model (GLM), using pre-whitening and an autoregressive model (AR-IRLS) [@barker2013] to reduce systemic physiology and motion-induced artifacts. Short-separation channels were used as regressors to further filter out physiological noise. A canonical hemodynamic response function (HRF) was assumed. A ROI analysis of the whole PFC is performed.

Analyses used a combined form of oxygenated (HbO) and deoxygenated (HbR) hemoglobin, in terms of correlation-based signal improvement (CBSI) [@cui2010]. While often used as a movement artifact correction method, this can be considered as a version of the oxygenated hemoglobin scaled for anticorrelation with the deoxygenated hemoglobin, incorporating information from both into one signal.

The second level (group) analysis is detailed in the analysis section.

# Process data

::: {.content-visible when-format="html"}
Code block - load data files

```{r}

# Load and prepare all data

var_data_path <- "../../Park-MOVE_fnirs_dataset_v2/IMU_data/imu_variability_parameters.csv"
gait_path <- "../../Park-MOVE_fnirs_dataset_v2/IMU_data//imu_gait_parameters.csv"
time_file <- paste("../../Park-MOVE_fnirs_dataset_v2/Task_data/auditory_stroop_answer_time.csv", sep="")
demo_file <- paste("../../Park-MOVE_fnirs_dataset_v2/basic_demographics.csv", sep="")
redcap_file <- paste("../../Park-MOVE_fnirs_dataset_v2/REDcap_data/All_REDcap_data.csv", sep="")
acc_file <- paste("../../Park-MOVE_fnirs_dataset_v2/Task_data/auditory_stroop_accuracy.csv", sep="")

# Which ROI?
selected_roi <- "PFC"

# How handle correlations?
corr_method <- "spearman"

# Do we filter any subjects?
filter_subjects <- c("NA")

### Identifiers
csv_path <- paste("../../Park-MOVE_fnirs_dataset_v2/identifiers_YA.csv", sep="")
identifiers_ya <- read.csv(csv_path)
csv_path <- paste("../../Park-MOVE_fnirs_dataset_v2/identifiers_OA.csv", sep="")
identifiers_oa <- read.csv(csv_path)
csv_path <- paste("../../Park-MOVE_fnirs_dataset_v2/identifiers_PD.csv", sep="")
identifiers_pd <- read.csv(csv_path)


```
:::

::: {.content-visible when-format="html"}
Code block - process data files, calculate dual-task effects, create subject means

```{r}

# Assign group function
assign_group <- function(df, identifiers_ya, identifiers_oa, identifiers_pd){
  df$group <- case_when(
    df$subject %in% identifiers_ya$id_nummer ~ "YA",
    df$subject %in% identifiers_oa$id_nummer ~ "OA",
    df$subject %in% identifiers_pd$id_nummer ~ "PD",
    TRUE ~ NA_character_
  )
  df$group <- factor(df$group, levels=c('YA', 'OA', 'PD'))
  return(df)
}

# Helper function to calculate DT cost
calculate_dt_cost <- function(dt, st) {
  -(dt - st) / st * 100
}

# Demographic data
demo_data <- read.csv(demo_file)
demo_data['sex'][demo_data['sex'] == 0] <- 'Male'
demo_data['sex'][demo_data['sex'] == 1] <- 'Female'
demo_data$sex <- factor(demo_data$sex, levels=c('Male', 'Female'))

# REDcap data
redcap_data <- read.csv(redcap_file) 
names(redcap_data)[names(redcap_data) == 'id_nummer'] <- 'subject'
redcap_data['ramlat_12_man'][redcap_data['ramlat_12_man'] == 0] <- 'No'
redcap_data['ramlat_12_man'][redcap_data['ramlat_12_man'] == 1] <- 'Yes'
redcap_data$ramlat_12_man <- factor(redcap_data$ramlat_12_man, levels=c('No', 'Yes'))

# Merge
all_subject_data <- merge(demo_data, redcap_data, by = "subject", all = TRUE)
all_subject_data <- assign_group(all_subject_data, identifiers_ya, identifiers_oa, identifiers_pd)

# Filter out YA
all_subject_data <- all_subject_data[!all_subject_data$group == 'YA', ]
all_subject_data$group <- factor(all_subject_data$group, levels=c('OA', 'PD'))

# Get TMT contrast
all_subject_data <- all_subject_data %>%
  mutate(tmt_4_tmt_2_contrast = tmt_4 - tmt_2)

# Read and process gait data
gait_data <- read.csv(gait_path)
gait_data <- gait_data %>%
  mutate(Cadence.LR = (Cadence.L + Cadence.R) / 2,
         Single.Support.LR = (Single.Support.L + Single.Support.R) / 2,
         Step.Count.LR = (Step.Count.L + Step.Count.R) / 2,
         Step.Time.LR = (Step.Time.L + Step.Time.R) / 2,
         Stride.Length.LR = (Stride.Length.L + Stride.Length.R) / 2,
         Walking.Speed.LR = (Walking.Speed.L + Walking.Speed.R) / 2,
         trial_type = recode(trial_type, 
                             'Navigation' = 'ST_navigation', 
                             'Straight_walking' = 'ST_walk', 
                             'Straight_walking_and_Aud_Stroop' = 'DT_walk', 
                             'Navigated_walking' = 'ST_navigation', 
                             'Navigation_and_Aud_Stroop' = 'DT_navigation')) %>%
  filter(trial_type != "Stand_still_and_Aud_Stroop")

# Get averages of all blocks per subject
gait_data <- gait_data %>%
  group_by(subject, session, trial_type) %>%
  summarise(mean_cadence = mean(Cadence.LR, na.rm = TRUE),
            mean_single_support = mean(Single.Support.LR, na.rm = TRUE), 
            mean_step_count = mean(Step.Count.LR, na.rm = TRUE),
            mean_step_time = mean(Step.Time.LR, na.rm = TRUE),
            mean_stride_length = mean(Stride.Length.LR, na.rm = TRUE),
            mean_walking_speed = mean(Walking.Speed.LR, na.rm = TRUE),
            .groups='drop')

# Make wider so we have one column per trial type
gait_data <- gait_data %>%
  pivot_wider(names_from = c(session, trial_type),
              values_from = c(mean_cadence, 
                              mean_single_support,
                              mean_step_count,
                              mean_step_time,
                              mean_stride_length,
                              mean_walking_speed))

# Read and process gait variability data
gait_variability_data <- read.csv(var_data_path) %>%
  mutate(trial_type = recode(trial_type, 'Navigation' = 'ST_navigation', 
                             'Straight_walking' = 'ST_walk', 
                             'Straight_walking_and_Aud_Stroop' = 'DT_walk', 
                             'Navigated_walking' = 'ST_navigation', 
                             'Navigation_and_Aud_Stroop' = 'DT_navigation'),
         Step.Time.Variability = Step.Time.Variability * 1000)

# Get averages of all blocks per subject
gait_variability_data <- gait_variability_data %>%
  group_by(subject, session, trial_type) %>%
  summarise(step_time_variability = mean(Step.Time.Variability, na.rm = TRUE),
            stride_length_variability = mean(Stride.Length.Variability, na.rm = TRUE), 
            step_time_asymmetry_percent = mean(Step.Time.Asymmetry.Percent, na.rm = TRUE),
            stride_length_asymmetry_percent = mean(Stride.Length.Asymmetry.Percent, na.rm = TRUE),
            .groups='drop')

# Make wider so we have one column per trial type
gait_variability_data <- gait_variability_data %>%
  pivot_wider(names_from = c(session, trial_type),
              values_from = c(step_time_variability, 
                              stride_length_variability,
                              step_time_asymmetry_percent,
                              stride_length_asymmetry_percent))
# Merge gait data
all_gait_data <- merge(gait_data, gait_variability_data, by = "subject", all = TRUE) %>%
  assign_group(identifiers_ya, identifiers_oa, identifiers_pd) %>%
  filter(group != 'YA')

# Add DT costs
all_gait_data <- all_gait_data %>%
  mutate(diff_walk_speed_protocol1 = mean_walking_speed_protocol1_DT_walk - mean_walking_speed_protocol1_ST_walk,
         diff_walk_speed_protocol3 = mean_walking_speed_protocol3_DT_navigation - mean_walking_speed_protocol3_ST_navigation,
         dt_cost_walk_speed_protocol1 = calculate_dt_cost(mean_walking_speed_protocol1_DT_walk, mean_walking_speed_protocol1_ST_walk),
         dt_cost_walk_speed_protocol3 = calculate_dt_cost(mean_walking_speed_protocol3_DT_navigation, mean_walking_speed_protocol3_ST_navigation),
         dt_cost_stride_length_protocol1 = calculate_dt_cost(mean_stride_length_protocol1_DT_walk, mean_stride_length_protocol1_ST_walk),
         dt_cost_stride_length_protocol3 = calculate_dt_cost(mean_stride_length_protocol3_DT_navigation, mean_stride_length_protocol3_ST_navigation),
         dt_cost_step_time_variability_protocol1 = calculate_dt_cost(step_time_variability_protocol1_DT_walk, step_time_variability_protocol1_ST_walk),
         dt_cost_step_time_variability_protocol3 = calculate_dt_cost(step_time_variability_protocol3_DT_navigation, step_time_variability_protocol3_ST_navigation))

# Process auditory stroop data
acc_data <- read.csv(acc_file)
acc_data_long <- pivot_longer(acc_data, 
                         cols = starts_with("accuracy_"), 
                         names_to = "accuracy_variable", 
                         values_to = "accuracy_value")

# Only take protocol 1
acc_data <- acc_data[acc_data$protocol == 'protocol_1', ]

# Make wider so we have one column per trial type
acc_data_wide <- acc_data[c('subject', 'block_type', 'protocol', 'accuracy_total')]
acc_data_wide <- acc_data_wide %>%
  pivot_wider(names_from = c(block_type, protocol),
              values_from = c(accuracy_total))

# Stroop times
time_data <- read.csv(time_file)

# Only take protocol 1
time_data <- time_data[time_data$protocol == 'protocol_1', ]
# One outlier in idx 7504
time_data <- time_data[-c(7504),]

# Assign group
time_data <- assign_group(time_data, identifiers_ya, identifiers_oa, identifiers_pd)
acc_data_long <- assign_group(acc_data_long, identifiers_ya, identifiers_oa, identifiers_pd)

# Get means per group
stroop_time_group_means <- time_data %>%
  group_by(group, block_type) %>%
  summarise(stroop_time_mean_value = mean(answer_time, na.rm = TRUE), .groups='drop')

# Get avg answer time per subject
stroop_time_subject_means <- time_data %>%
  group_by(subject, protocol, block_type) %>%
  summarise(stroop_time_mean_value = mean(answer_time, na.rm = TRUE), .groups='drop')

# Make wider so we have one column per trial type
stroop_time_subject_means_wide <- stroop_time_subject_means %>%
  pivot_wider(names_from = c(block_type, protocol),
              values_from = c(stroop_time_mean_value))

# Add DT costs for stroop time
stroop_time_subject_means_wide <- stroop_time_subject_means_wide %>%
  mutate(dt_cost_stroop_time = -calculate_dt_cost(DT_protocol_1, ST_protocol_1))

# Merge with accuracy
as_data_wide <- merge(acc_data_wide, stroop_time_subject_means_wide, by = "subject", all = TRUE, suffixes = c("_acc", "_time"))

# Add group again
as_data_wide <- assign_group(as_data_wide, identifiers_ya, identifiers_oa, identifiers_pd)
as_data_wide <- as_data_wide[!as_data_wide$group == 'YA', ]


# Split into different groups
all_subject_data <- merge(all_subject_data, all_gait_data, by = "subject", all = TRUE)
all_subject_data <- merge(all_subject_data, as_data_wide[c("subject", "dt_cost_stroop_time")], by = "subject", all = TRUE)
all_subject_data <- subset(all_subject_data, select = -c(group.y))
names(all_subject_data)[names(all_subject_data) == 'group.x'] <- 'group'
ya_data <- all_subject_data[all_subject_data$group == 'YA', ]
ya_data <- ya_data[!is.na(ya_data$subject),]
oa_data <- all_subject_data[all_subject_data$group == 'OA', ]
oa_data <- oa_data[!is.na(oa_data$subject),]
pd_data <- all_subject_data[all_subject_data$group == 'PD', ]
pd_data <- pd_data[!is.na(pd_data$subject),]

```
:::

# Demographics and gait variables

Demographic data and task performance data was compared between groups using R (v4.2.2) (R Core Team, 2022) [@rcoreteam2022] and the arsenal (v3.6.3) package. Normality was assessed with the Shapiro-Wilk normality test and visually with q-q plots. Comparison was done with the Kruska-Wallis test.

::: {.content-visible when-format="html"}
```{r, results='asis'}

# https://cran.r-project.org/web/packages/arsenal/vignettes/tableby.html

# Cleaner names
labels(all_subject_data)  <- c(
  age = "Age, yrs", 
  sex = "Gender, female",
  crf_utbildning_ar = "Education, yrs",
  weight = "Weight, kg",
  height = "Height, cm",
  frandin_grimby.x = "Frändin-Grimby",
  ramlat_12_man = "Falls in last 12 months, yes",
  mb_total = "Mini-BESTest score",
  g12_sum = "Walk-12 sum",
  tmt_2 = "TMT2, s",
  tmt_4 = "TMT4, s",
  tmt_4_tmt_2_contrast = "TMT4 - TMT2, s",
  cwit_3 = "CWIT3, s",
  ravlt_ret = "RAVLT retention, words")

labels(all_gait_data)  <- c(
  dt_cost_walk_speed_protocol1 = "DT cost walking speed, %",
  dt_cost_walk_speed_protocol3 = "DT cost walking speed, %",
  dt_cost_stride_length_protocol1 = "DT cost stride length, %",
  dt_cost_stride_length_protocol3 = "DT cost stride length, %",
  dt_cost_step_time_variability_protocol1 = "DT cost step time variability, %",
  dt_cost_step_time_variability_protocol3 = "DT cost step time variability, %",
  mean_walking_speed_protocol1_ST_walk = "Walking speed (ST), m/s",
  mean_walking_speed_protocol1_DT_walk = "Walking speed (DT), m/s",
  mean_walking_speed_protocol2_ST_walk = "Walking speed (straight), m/s",
  mean_walking_speed_protocol2_ST_navigation = "Walking speed (navigation), m/s",
  mean_walking_speed_protocol3_ST_navigation = "Walking speed (ST), m/s",
  mean_walking_speed_protocol3_DT_navigation = "Walking speed (DT), m/s",
  mean_stride_length_protocol1_ST_walk = "Stride length (ST), m",
  mean_stride_length_protocol1_DT_walk = "Stride length (DT), m",
  mean_stride_length_protocol2_ST_walk = "Stride length (straight), m",
  mean_stride_length_protocol2_ST_navigation = "Stride length (navigation), m",
  mean_stride_length_protocol3_ST_navigation = "Stride length (ST), m",
  mean_stride_length_protocol3_DT_navigation = "Stride length (DT), m",
  step_time_variability_protocol2_ST_walk = "Step time variability (straight), ms",
  step_time_variability_protocol1_ST_walk = "Step time variability (ST), ms",
  step_time_variability_protocol1_DT_walk = "Step time variability (DT), ms",
  step_time_variability_protocol2_ST_navigation = "Step time variability (navigation), ms",
  step_time_variability_protocol3_ST_navigation = "Step time variability (ST), ms",
  step_time_variability_protocol3_DT_navigation = "Step time variability (DT), ms",
  turns_velocity_protocol_2 = "Turn velocity (peak), deg/s",
  turns_velocity_protocol_3 = "Turn velocity (peak), deg/s")

# Properties for all tables
mycontrols  <- tableby.control(numeric.test="kwt", cat.test="chisq",  cat.simplify = TRUE, cat.stats="countpct",
                               numeric.simplify=TRUE, numeric.stats=c("meansd"),
                               digits=2, digits.p=2, digits.pct=1)

# No good function for creating sub-headings in the table
# But we could create separate tables for display for now (and then all together for correct statistics later)
tab1 <- tableby(group ~ sex + age + crf_utbildning_ar + weight + height + frandin_grimby.x + ramlat_12_man,
                data=all_subject_data, control=mycontrols)


tab2 <- tableby(group ~ mb_total + g12_sum + tmt_2 + tmt_4 + tmt_4_tmt_2_contrast + cwit_3 + ravlt_ret,
                data=all_subject_data, control=mycontrols)



tab3 <- tableby(group ~
                mean_walking_speed_protocol1_ST_walk +
                mean_walking_speed_protocol1_DT_walk + 
                mean_stride_length_protocol1_ST_walk +
                mean_stride_length_protocol1_DT_walk + 
                step_time_variability_protocol1_ST_walk +
                step_time_variability_protocol1_DT_walk +
                dt_cost_walk_speed_protocol1 + 
                dt_cost_stride_length_protocol1 +
                dt_cost_step_time_variability_protocol1,
                data=all_gait_data, control=mycontrols)

tab4 <- tableby(group ~ 
                mean_walking_speed_protocol2_ST_walk +
                mean_walking_speed_protocol2_ST_navigation +
                mean_stride_length_protocol2_ST_walk +
                mean_stride_length_protocol2_ST_navigation + 
                step_time_variability_protocol2_ST_walk +
                step_time_variability_protocol2_ST_navigation,
                data=all_gait_data, control=mycontrols)


tab5 <- tableby(group ~ 
                mean_walking_speed_protocol3_ST_navigation +
                mean_walking_speed_protocol3_DT_navigation + 
                mean_stride_length_protocol3_ST_navigation +
                mean_stride_length_protocol3_DT_navigation + 
                step_time_variability_protocol3_ST_navigation +
                step_time_variability_protocol3_DT_navigation +
                dt_cost_walk_speed_protocol3 + 
                dt_cost_stride_length_protocol3 + 
                dt_cost_step_time_variability_protocol3,
                data=all_gait_data, control=mycontrols)


labels(as_data_wide)  <- c(
  ST_protocol_1_acc = "Accuracy (ST), %",
  DT_protocol_1_acc = "Accuracy (DT), %",
  DT_protocol_1_time = "Answer time (DT), s",
  ST_protocol_1_time = "Answer time (ST), s",
  dt_cost_stroop_time = "DT cost answer time, %")
  
tab6 <- tableby(group ~ ST_protocol_1_acc + ST_protocol_1_time +
                DT_protocol_1_acc + DT_protocol_1_time + dt_cost_stroop_time,
                data=as_data_wide, control=mycontrols)


```
:::

```{r, results='asis'}
summary(tab1, title = "demographics")
summary(tab2, title = "clinical & neuropsychological")
summary(tab3, title = "Gait variables, protocol 1")
summary(tab6, title = "Auditory Stroop, protocol 1")
```

# Descriptive analysis of dual-task effects

To illustrate patterns of interference and prioritization during dual-tasking, cognitive and motor dual-task effects were calculated and plotted against each other. Quadrants in this plot can be used to categorize dual-task effects in terms of mutual interference, cognitive or motor priority trade offs, or mutual facilitation [@plummer2013]. The resulting diagram for dual-task effect on walking speed and response time for the auditory Stroop task can be seen in Figure 1. A negative dual-task effect indicates a dual-task cost (i.e., worse performance during dual-tasking) and a positive dual-task effect indicates a dual-task benefit (i.e., better performance during dual-tasking).

Most participants had a cognitive priority trade-off when dual-tasking (47%), which means they responded as fast or faster to the Stroop task, but walked slower when dual-tasking. Interesting to note is that this is the “posture second” strategy observed in [@bloem2006]. Most participants had dual-task motor costs (i.e., they walked slower) but some older adults both walked faster and responded faster, ending up in the mutual facilitation quadrant.

Combined with the task data in the tables above, where we can observe a significantly higher dual-task cost on walking speed for the PD group but similar performance on the Stroop task, we find that the PD group has a dual-task impairment that might not be able to be explained by just prioritization. Thus, models of gait automaticity compensation might be extra interesting in this group

## Quadrant

::: {.content-visible when-format="html"}
```{r}
#| warning: false

# Merge dt costs and stroop time on subject
dt_costs <- all_gait_data %>%
  select(starts_with("dt_cost"), subject)
merged_data <- merge(as_data_wide, dt_costs, by = 'subject')

# Make costs into DTE instead
merged_data <- merged_data %>%
  mutate(across(starts_with("dt_cost"), ~ . * -1))

merged_data$quadrant <- with(merged_data, 
                             ifelse(dt_cost_stroop_time > 0 & dt_cost_walk_speed_protocol1 > 0, "Q1",
                             ifelse(dt_cost_stroop_time < 0 & dt_cost_walk_speed_protocol1 > 0, "Q2",
                             ifelse(dt_cost_stroop_time < 0 & dt_cost_walk_speed_protocol1 < 0, "Q3", "Q4"))))

quadrant_counts <- table(merged_data$quadrant)

# Convert counts to percentages
quadrant_percentages <- round(prop.table(quadrant_counts) * 100, 1)

# Create quadrant plot
p <- ggplot(merged_data, aes(x = dt_cost_stroop_time, y = dt_cost_walk_speed_protocol1, tooltip = subject, color = group)) +
  geom_point_interactive() +
  labs(x = 'DTE stroop time (%)', y = 'DTE walk speed (%)') +
  theme_minimal() +
  theme(text = element_text(size = 14)) +  # Increase text size
  geom_quadrant_lines(xintercept = 0, yintercept = 0, linetype = "dashed", color = "black") +
  xlim(-20, 20) +
  ylim(-20, 20) +
  annotate("text", x = 13, y = 17, label = paste("Mutual facilitation %:", quadrant_percentages["Q1"]), color = "blue", size = 5) +
  annotate("text", x = -13, y = 17, label = paste("Motoric priority trade-off %:", quadrant_percentages["Q2"]), color = "blue", size = 5) +
  annotate("text", x = -13, y = -17, label = paste("Mutual interference %:", quadrant_percentages["Q3"]), color = "blue", size = 5) +
  annotate("text", x = 13, y = -17, label = paste("Cognitive priority trade-off %:", quadrant_percentages["Q4"]), color = "blue", size = 5)

# Render the plot
p


```
:::

# Planned analysis (proposed model)

## Aim 1:

To test the basic assumption that compensatory cortical activity occurs to compensate for a loss of gait automaticity, evaluate whether a measure of gait automaticity (step time variability) relates to cortical (prefrontal) activity, within each group (OA and PD).

**Expectation:**

A loss of gait automaticity (assumed to happen in both aging and Parkinson’s disease) should incur compensatory cortical activity, in accordance with upregulation and/or reorganization from the CRUNCH model. Therefore, a loss of gait automaticity should co-occur with (i.e., be related to) an increase in cortical activity.

We will use step time variability as a measure of locomotor automaticity in accordance with the arguments put forth by [@gilat2017].

In the model below, we would thus expect a significant positive parameter estimate for step_time_var. The model also controls for age. If we find a relationship, we could plot the relationship between step time variability and beta (and possibly other variables in an exploratory fashion). See the section @sec-power-sample-size for sensitivity analysis and effect size considerations.

**Model:**

```         
beta ~ -1 + condition + step_time_var + age + (disease_severity)
```

![Expected relationship, model 1, with some simulated values](notebook-resources/model_1.JPG)

## Aim 2:

Explore the association between different dual-task components (gait and cognition) and prioritization and measured PFC activity. Not sure about the relationships here, but interesting to investigate.

**Models:**

```         
beta ~ -1 + condition + DTC_gait + age + (disease_severity)
```

```         
beta ~ -1 + condition + DTC_cog + age + (disease_severity)
```

```         
beta ~ -1 + condition + prioritization + age + (disease_severity)
```

## Aim 3:

Evaluate whether and how cognition moderates the automaticity / brain activation relationship. Increased cognitive ability may influence the need for (degree of) cortical activity in the face of reduced automaticity.

**Expectation:**

In the literature, we find that dual-task impairments are exacerbated by reduced executive function. One interpretation of this is that people with reduced executive function have lower capacity for compensation. Thus, we could expect a measure of executive function (TMT B) to affect the compensation relationship (i.e., between reduced automaticity and increased cortical activity).

*Behavioral*: if dual-task impairments are exacerbated by reduced executive function, we would expect a significant positive relationship between DT cost (motor or cognitive cost), and TMT-B completion time (longer completion time, worse executive ability). Run a simple Pearson correlation between TMT-B time and DT cost.

*Neurophysiological*: this one is more unclear. If we think **reduced executive ability** might **diminish** the brain’s ability to compensate for reduced automaticity (at least via the PFC, I am interpreting Bunzeck et al section 5.2), then EF could act as a moderator in the compensation relationship between automaticity and PFC activity. In the model below, the interaction *step_time_var:tmt_b* would then be significant, probably in the negative direction (longer TMT time = worse EF, less compensation = lower beta). See plot.

**Model:**

```         
beta ~ -1 + condition + step_time_var * tmt_b + age + education_years + (disease_severity)
```

or written out fully

```         
beta ~ -1 + condition + step_time_var + tmt_b + step_time_var:tmt_b + age + education_years + (disease_severity)
```

![Expected relationship(s), aim 3, with some simulated values](notebook-resources/model_2_combined.png)

## Model details

Use a linear fixed effects regression model. Also include the effect of walking condition in the model. Include age in the model, to control for the pretty large age span within our groups. Models would be solved in the NIRS Brain AnalyzIR toolbox. Consider significance FDR-adjusted p \< 0.05.

Variables:

```         
beta - subject-level ROI estimate of hemoglobin activation from subject-level GLM (CBSI estimate, i.e. a linear combination of oxy and deoxy, scaled by anticorrelation) 

condition - factor with 3 levels (standing audio stroop, walking, dual-task walking) 

step_time_var - subject average step time variability calculated from single-task walking blocks 

tmt_b - time to complete the TMT B for the subject 

DTC_gait - dual-task cost on walking speed 

DTC_cog - dual-task cost on audio stroop reaction time prioritization - 

DTC_gait - DTC_cog

education_years – number of years of education for participant

disease_severity – variable for disease severity for the PD group, determined by likelihood ratio test – LEDD, MDS-UPDRS3 motor score, disease length
```

Then check if mathematical assumptions of the model have been fulfilled with diagnostic plots: normality of residuals / homogeneity of variance (q-q of residuals, residuals vs fitted), influential outliers (residual vs leverage). If a model does not hold, investigate outliers or change model. Document these steps in an ‘open lab notebook’ approach.

## Power and sample size – sensitivity analyses {#sec-power-sample-size}

### Primary analysis (aim 1)

Here we want to test that the inclusion of the covariate step_time_var significantly increases the proportion of variance of beta explained by the model (with 3 predictors total).

In GPower:

F tests – Linear multiple regression: Fixed model, R\^2 increase

α = 0.05, 1-β = 0.80, sample size = 42 (PD) -\> effect size = 0.197

α = 0.05, 1-β = 0.80, sample size = 49 (OA) -\> effect size = 0.167

Note that this resulting graph is the same as two-tailed “t tests – Linear multiple regression: fixed model, single regression coefficient”.

These are between medium and large effect sizes going by Cohen’s definitions, defined as: small f2 = 0.02, medium f2 = 0.15, large f2 = 0.35.

This means that we could reliably make conclusions about effects of medium to large size, but not smaller than that. A null result could therefore mean that there might still be a small effect that our sample size is too small to detect. A smallest effect size of interest (SESOI) is difficult to speculate on. However, seeing if there is a medium to large effect is still interesting.

![Effect size / sample size curve for model 1 (F test)](notebook-resources/gpower_1.png)

![Effect size / sample size curve for model 1 (t test)](notebook-resources/gpower_2.png)

### Secondary analysis (aim 2 & 3)

For the effects of DTC_cog, DTC_gait, and prioritization, the sensitivity analysis would be as above for aim 1. For the moderation model, we would test the interaction effect.

```         
beta ~ -1 + condition + step_time_var * tmt_b + age
```

Estimating the interaction effect is more involved than main effects. Using InteractionPoweR [@baranger2023], and setting the following variables:

```         
Letting y = beta, x1 = step time variability, x2 = TMT B score 

alpha = 0.05 Correlation between x1 and y = 0.2 

Correlation between x2 and y = 0.2 

Correlation between x1 and x2 = 0.2 

N = 42
```

These correlations are assumed and could be very different in reality.

Then, to reach a power of 0.80, we would need an interaction effect size of 0.45. However, according to recommendations [@baranger2023], one should generally expect the interaction effect size to be smaller than the main effect sizes. Thus, it is unlikely that we would detect this interaction effect. It can be considered entirely exploratory. Consider it a last priority.

## How has the data already been used?

We have a pilot study (<https://doi.org/10.1002/brb3.2948)> where we analysed the first 10 participants and looked at some condition effects. We have a validation study (<https://doi.org/10.1016/j.nicl.2024.103637)> where we looked at condition effects (single-task and dual—task walking) as well as a number of interaction effects (including the interaction condition:step_time_variability, which was significant for the PD group). In the validation article, we limited analysis to dorsolateral PFC, not the entire PFC. We also did not cut out the initiation phases of the gait variables, nor controlled for age in analysed models. Here, we are using different models and looking at different effects (not the interaction effects as in the validation article).

# Performed analysis

Subject-level fNIRS analysis can be found in [park_move_protocol_1_subject_analysis.m](https://github.com/alkvi/fnirs_stroop_study/blob/main/park_move_protocol_1_subject_analysis.m)

Group-level fNIRS analysis can be found in [park_move_protocol_1_group_analysis.m](https://github.com/alkvi/fnirs_stroop_study/blob/main/park_move_protocol_1_group_analysis.m)

Missing data compared to original targets are:

-   3 OA (1 after re-evaluating criteria, 2 not following protocol), 2 were also missing gait data (data loss during protocol).
-   3 PD (1 data loss, 1 not following protocol, 1 not completing protocol), 2 were also missing MDS-UPDRS data.
-   Amount of included subjects in each model is indicated in included_n.

The disease_severity variable in the PD models was decided as MDS-UPDRS 3 motor score, after comparing the Akaike Information Criterion (AIC) for models utilizing each variable (and LEDD was skipped due to too many missing values).

The group-level results (output from linear models) is exported to csv and read below.

```{r}


# Load fNIRS model results
group_stats_file <- "../data/results_table_cbsi_protocol_1.csv"
group_stats <- read.csv(group_stats_file)
group_stats <- group_stats %>%
  select(-AIC, -BIC, -formula, -type, -ROI, -q) %>%
  rename(Condition = Contrast,
         Aim = comment) %>%
  mutate(Condition = str_replace_all(Condition, c("Stand_still_and_Aud_Stroop" = "ST_stand", 
                                        "Straight_walking" = "ST_walk",
                                        "Straight_walking_and_Aud_Stroop" = "DT_walk",
                                        "step_time_var" = "Step time variability",
                                        "3_motor" = "UPDRS 3 Motor")))
  

# FDR-Adjust p-values per aim so both groups are included
table_aim_1 <- group_stats %>% filter(Aim == "Aim 1")
table_aim_1$p_adjust <- p.adjust(table_aim_1$p, method="BH")
table_aim_1 <- table_aim_1 %>% select(1:6, p_adjust, everything())

table_aim_2 <- group_stats %>% filter(str_detect(Aim, "Aim 2"))
table_aim_2$p_adjust <- p.adjust(table_aim_2$p, method="BH")
table_aim_2 <- table_aim_2 %>% select(1:6, p_adjust, everything())
table_aim_2_1 <- table_aim_2 %>% filter(Aim == "Aim 2_1") 
table_aim_2_2 <- table_aim_2 %>% filter(Aim == "Aim 2_2") 
table_aim_2_3 <- table_aim_2 %>% filter(Aim == "Aim 2_3") 

table_aim_3 <- group_stats %>% filter(Aim == "Aim 3")
table_aim_3$p_adjust <- p.adjust(table_aim_3$p, method="BH")
table_aim_3 <- table_aim_3 %>% select(1:6, p_adjust, everything())

```

## Results, aim 1

```{r}

table_aim_1 %>%
  gt() %>%
  fmt_number(
    columns = c(included_n, DF),
    decimals = 0
  ) %>%
  fmt_number(
    columns = -c(included_n, DF),
    decimals = 3
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(
      rows = p_adjust < 0.05
    )
  ) %>%
  tab_row_group(
    label = md("**Group: OA**"),
    rows = group == "OA"
  ) %>%
  tab_row_group(
    label = md("**Group: PD**"),
    rows = group == "PD"
  )

```

## Results, aim 2

```{r}

table_aim_2_1 %>%
  gt() %>%
  fmt_number(
    columns = c(included_n, DF),
    decimals = 0
  ) %>%
  fmt_number(
    columns = -c(included_n, DF),
    decimals = 3
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(
      rows = p_adjust < 0.05
    )
  ) %>%
  tab_row_group(
    label = md("**Group: OA**"),
    rows = group == "OA"
  ) %>%
  tab_row_group(
    label = md("**Group: PD**"),
    rows = group == "PD"
  )

table_aim_2_2 %>%
  gt() %>%
  fmt_number(
    columns = c(included_n, DF),
    decimals = 0
  ) %>%
  fmt_number(
    columns = -c(included_n, DF),
    decimals = 3
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(
      rows = p_adjust < 0.05
    )
  ) %>%
  tab_row_group(
    label = md("**Group: OA**"),
    rows = group == "OA"
  ) %>%
  tab_row_group(
    label = md("**Group: PD**"),
    rows = group == "PD"
  )

table_aim_2_3 %>%
  gt() %>%
  fmt_number(
    columns = c(included_n, DF),
    decimals = 0
  ) %>%
  fmt_number(
    columns = -c(included_n, DF),
    decimals = 3
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(
      rows = p_adjust < 0.05
    )
  ) %>%
  tab_row_group(
    label = md("**Group: OA**"),
    rows = group == "OA"
  ) %>%
  tab_row_group(
    label = md("**Group: PD**"),
    rows = group == "PD"
  )

```

## Results, aim 3

### Behavioral

Note that this is Spearman correlation instead of planned Pearson, because there are outliers in the PD data that we want the statistics to be robust against.

::: column-page
```{r}
#| warning: false

## TMT4 - DT cost

gg_point_1 = ggplot(data = oa_data, aes(x = tmt_4, y = dt_cost_walk_speed_protocol1)) +
  geom_point_interactive(aes(tooltip = subject, data_id = subject), color="red") +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  stat_cor(label.y = 14, method = corr_method, size = 4) +
  coord_cartesian(xlim =c(20,235), ylim = c(-10, 15)) +
  labs(x = "TMT-B", y = "DT cost speed (%)", title = "OA") +
  geom_hline(yintercept=0, linetype="dashed", color = "black") +
  theme_minimal()

gg_point_2 = ggplot(data = pd_data, aes(x = tmt_4, y = dt_cost_walk_speed_protocol1)) +
  geom_point_interactive(aes(tooltip = subject, data_id = subject), color = "green") +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  stat_cor(label.y = 14, method = corr_method, size = 4) +
  coord_cartesian(xlim =c(20,235), ylim = c(-10, 15)) +
  labs(x = "TMT-B", y = "DT cost speed (%)", title = "PD") +
  geom_hline(yintercept=0, linetype="dashed", color = "black") +
  theme_minimal()

## TMT4 - stoop DT cost

gg_point_3 = ggplot(data = oa_data, aes(x = tmt_4, y = dt_cost_stroop_time)) +
  geom_point_interactive(aes(tooltip = subject, data_id = subject), color="red") +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  stat_cor(label.y = 14, method = corr_method, size = 4) +
  coord_cartesian(xlim =c(20,235), ylim = c(-10, 15)) +
  labs(x = "TMT-B", y = "DT cost stroop (%)", title = "OA") +
  geom_hline(yintercept=0, linetype="dashed", color = "black") +
  theme_minimal()

gg_point_4 = ggplot(data = pd_data, aes(x = tmt_4, y = dt_cost_stroop_time)) +
  geom_point_interactive(aes(tooltip = subject, data_id = subject), color = "green") +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  stat_cor(label.y = 14, method = corr_method, size = 4) +
  coord_cartesian(xlim =c(20,235), ylim = c(-10, 15)) +
  labs(x = "TMT-B", y = "DT cost stroop (%)", title = "PD") +
  geom_hline(yintercept=0, linetype="dashed", color = "black") +
  theme_minimal()


plot_grid(plotlist=list(gg_point_1, gg_point_2, gg_point_3, gg_point_4), ncol=2, nrow=2)


```
:::

### fNIRS

```{r}

table_aim_3 %>%
  gt() %>%
  fmt_number(
    columns = c(included_n, DF),
    decimals = 0
  ) %>%
  fmt_number(
    columns = -c(included_n, DF),
    decimals = 3
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_body(
      rows = p_adjust < 0.05
    )
  ) %>%
  tab_row_group(
    label = md("**Group: OA**"),
    rows = group == "OA"
  ) %>%
  tab_row_group(
    label = md("**Group: PD**"),
    rows = group == "PD"
  )

```

# Model validation

Models were exported from MATLAB and evaluated in R to test assumptions for linearity, homoscedasticity and normality of residuals. Influential outliers were also analyzed.

The residuals are from the NIRS toolbox GroupStats models (the linear models on group level). The number of residuals represent (channels) x (subjects) x (conditions/covariates).

Model validation plots can be found here: [Model validation](model_diagnostics.html)

## Diagnostics

As far as I can tell, mostly for the OA group, the residuals have somewhat "fat-tailed" distributions. This would make results in the OA group slightly anticonservative. The PD model distributions look more normally distributed. Residual-Fitted plots look symmetrically distributed.

## Outlier analysis

Outliers were evaluated in the Residual-Leverage and influence plots in [model validation](model_diagnostics.html), and by using the [RemoveOutlierSubjects.m](https://github.com/huppertt/nirs-toolbox/blob/56e1487d309757af54fd39d9241d173356cabbf9/%2Bnirs/%2Bmodules/RemoveOutlierSubjects.m#L2) function in the NIRS toolbox.

There were a few outliers, more in the OA group than in the PD group. The RemoveOutlierSubjects.m function showed 2 outlier subjects in OA, and 1 in PD. These corresponded to large circles in the influence plots.

Re-running the analysis without the outlier subjects resulted in slightly different beta values, but significant effects in each aim remained mostly the same, except a few condition effects (ST_stand for OA in aim 1, DT_walk for PD in aim 2, ST_stand for OA in aim 3). The difference were in those condition effects which were hovering around FDR-adjusted p .05.

This points to robust results for the effects of interest (step time variability, age, DT costs), and mostly robust results for condition effects.

# Exploratory - PFC activity visualization and comparison

## Visualization

Below is PFC activity per channel visualized for OA, PD, and the contrast PD - OA.

Color scale indicates T value. Red indicates an increase in BOLD activity for a channel, blue a decrease. The increase/decrease is always from baseline (standing still without a task) to active condition.

In the contrasts, blue thus means OA \> PD and red means PD \> OA.

Significant increases (FDR p \< .05) are indicated as solid lines.

**Condition 1 - standing auditory stroop**

A few significant channels in each group: decrease for OA and increase for PD. No channel in the contrast had a significant diffrence on its own.

::: {layout-ncol="3"}
![OA](../figures/Stand_still_and_Aud_Stroop__group_OA_cbsi.png)

![PD](../figures/Stand_still_and_Aud_Stroop__group_PD_cbsi.png)

![Contrast](../figures/-Stand_still_and_Aud_Stroop__group_OA+Stand_still_and_Aud_Stroop__group_PD_cbsi.png)
:::

**Condition 2 - straight walking**

OA had more of an increase than PD.

::: {layout-ncol="3"}
![OA](../figures/Straight_walking__group_OA_cbsi.png)

![PD](../figures/Straight_walking__group_PD_cbsi.png)

![Contrast](../figures/-Straight_walking__group_OA+Straight_walking__group_PD_cbsi.png)
:::

**Condition 3 - dual-task walking**

OA had more of an increase than PD.

::: {layout-ncol="3"}
![OA](../figures/Straight_walking_and_Aud_Stroop__group_OA_cbsi.png)

![PD](../figures/Straight_walking_and_Aud_Stroop__group_PD_cbsi.png)

![Contrast](../figures/-Straight_walking_and_Aud_Stroop__group_OA+Straight_walking_and_Aud_Stroop__group_PD_cbsi.png)
:::

## Contrast ROI

The following model on increase in PFC activity per group was run:

```         
beta ~ -1 + condition:group
```

Then, the contrast PD - OA was taken for each condition. Results are in the table below.

Results are as seen above: OA had more PFC activity in the walking conditions, while PD had more in the standing auditory stroop condition.

```{r}
#| warning: false

# Read contrasts
contrast_file <- "../data/results_table_cbsi_protocol_1_contrast.csv"
contrast_data <- read.csv(contrast_file)
contrast_data <- contrast_data %>%
  select(-type, -ROI, -q) %>%
  rename(Condition = Contrast)

# Nicer names
contrast_data$Condition <- gsub("-Stand_still_and_Aud_Stroop:group_OA\\+Stand_still_and_Aud_Stroop:group_PD", "ST_stand",
                                contrast_data$Condition)
contrast_data$Condition <- gsub("-Straight_walking:group_OA\\+Straight_walking:group_PD", "ST_walk", 
                                contrast_data$Condition)
contrast_data$Condition <- gsub("-Straight_walking_and_Aud_Stroop:group_OA\\+Straight_walking_and_Aud_Stroop:group_PD", "DT_walk",
                                contrast_data$Condition)
# Display
contrast_data %>%
  gt() %>%
  fmt_number(
    columns = everything(),
    decimals = 3
  )
  
```

# Signal quality

fNIRS signal quality in each condition was calculated using MNE-NIRS and QT-NIRS: <https://github.com/alkvi/fnirs_dataset_preparation>

```{r}
#| warning: false

quality_path_sci <- "../data/signal_quality_sci.csv"
quality_path_psp <- "../data/signal_quality_power.csv"
sci_data <- read.csv(quality_path_sci)
psp_data <- read.csv(quality_path_psp)

# Select protocol 1
sci_data <- subset(sci_data, protocol == "protocol1")
psp_data <- subset(psp_data, protocol == "protocol1")

# Get averages of all blocks per subject
psp_data_avg <- psp_data %>%
  group_by(protocol, condition) %>%
  summarise(mean_power = mean(power, na.rm = TRUE),
            mean_power_std = mean(power_std, na.rm = TRUE),
            .groups='drop')

# Add SCI
quality_data <- merge(psp_data_avg, sci_data, by = "condition", all = TRUE)
quality_data <- quality_data[c("condition", "mean_power", "mean_power_std", "sci", "sci_std")]
names(quality_data)[names(quality_data) == "mean_power"] <- "Peak Spectral Power"
names(quality_data)[names(quality_data) == "mean_power_std"] <- "Peak Spectral Power (STD)"
names(quality_data)[names(quality_data) == "sci"] <- "Scalp Coupling Index"
names(quality_data)[names(quality_data) == "sci_std"] <- "Scalp Coupling Index (STD)"

# Display
quality_data %>%
  gt() %>%
  fmt_number(
    columns = everything(),
    decimals = 3
  )
  
```

# References
